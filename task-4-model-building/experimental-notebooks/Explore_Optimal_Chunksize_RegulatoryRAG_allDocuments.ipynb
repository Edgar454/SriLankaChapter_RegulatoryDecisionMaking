{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmdenaAI/SriLankaChapter_RegulatoryDecisionMaking/blob/main/task-4-model-building/experimental-notebooks/Explore_Optimal_Chunksize_RegulatoryRAG_allDocuments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "4hyanJvqZzlP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "81FquzcWSdlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb95aa3-4ef4-42fe-f932-9bea5f97cc42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.1/233.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install llama-index -q\n",
        "%pip install transformers -q\n",
        "%pip install torch -q\n",
        "%pip install llama-index-llms-groq -q\n",
        "%pip install sentence-transformers -q\n",
        "%pip install \"llama-index-embeddings-huggingface\" -q\n",
        "%pip install kdbai-client -q\n",
        "%pip install llama-index-vector-stores-kdbai -q\n",
        "%pip install kdbai_client -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from llama_index.core import VectorStoreIndex, ServiceContext, Document\n",
        "from llama_index.core.node_parser import SentenceSplitter, MarkdownNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core.llms import ChatMessage\n",
        "import kdbai_client as kdbai\n",
        "\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hxsO6P24TFnJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "CJ0qj3DVZ50B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(csv_path: str, text_col: List[str], metadata_cols: List[str]) -> List[Document]:\n",
        "  \"\"\"\n",
        "  Load documents and include class in metadata\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(csv_path)\n",
        "  df.fillna(\"nan\", inplace=True)\n",
        "  documents = []\n",
        "  cols = ['original_doc_id', 'class', 'issuing_authority', 'title', 'issue_date', 'reference_number']\n",
        "  for _, row in df.iterrows():\n",
        "      text = str(row[text_col])\n",
        "      doc = Document(\n",
        "          text=text,\n",
        "          metadata= {cols[i]: row[col] for i, col in enumerate(metadata_cols)}\n",
        "      )\n",
        "      documents.append(doc)\n",
        "  return documents\n",
        "\n",
        "DATA_PATH = \"https://raw.githubusercontent.com/OmdenaAI/SriLankaChapter_RegulatoryDecisionMaking/main/data/final_dataset/v0_LK_tea_dataset.csv\"\n",
        "# DATA_PATH = \"https://github.com/OmdenaAI/SriLankaChapter_RegulatoryDecisionMaking/blob/main/data/final_dataset/v0_LK_tea_dataset.csv\"\n",
        "text_col = 'markdown_content'\n",
        "metadata_cols = ['id', 'class', 'issuing_authority', 'llama_title', 'llama_issue_date', 'llama_reference_number']\n",
        "\n",
        "all_documents = load_data(DATA_PATH, text_col, metadata_cols)\n",
        "len(all_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVfXRRS5THRw",
        "outputId": "692145d6-b940-4b8f-b639-839afeebd14a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate ideal chunk size for the RAG system"
      ],
      "metadata": {
        "id": "43K3OkNSwTGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.core.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import time\n",
        "\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "oM87W7ahwNkG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate questions to be used for query"
      ],
      "metadata": {
        "id": "fqXdgmk-w5VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To evaluate for each chunk size, generate 10 questions from the input documents\n",
        "eval_documents = all_documents\n",
        "data_generator = DatasetGenerator.from_documents(eval_documents)\n",
        "eval_questions = data_generator.generate_questions_from_nodes(num = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtCXXw-fwvgv",
        "outputId": "6390f7ef-ad01-4e9e-8378-05d97ab9739a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/llama_index/core/evaluation/dataset_generation.py:200: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
            "  return cls(\n",
            "/usr/local/lib/python3.10/dist-packages/llama_index/core/evaluation/dataset_generation.py:296: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
            "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3QwnPDG0TBI",
        "outputId": "c43c8cfa-2516-421a-945d-9542bd479e0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is the title of the regulatory act mentioned in the context information?',\n",
              " 'When was the Tea Subsidy Act, No. 12 of 1958 issued?',\n",
              " 'Who is the issuing authority of the Tea Subsidy Act?',\n",
              " 'What is the main purpose of the Tea Subsidy Act?',\n",
              " 'What is the reference number of the Tea Subsidy Act?',\n",
              " 'What does the Tea Subsidy Act aim to subsidize?',\n",
              " 'What is the significance of establishing a fund as mentioned in the Tea Subsidy Act?',\n",
              " 'What is the export duty imposed by the Tea Subsidy Act?',\n",
              " 'How does the Tea Subsidy Act support the replanting and rehabilitation of tea estates and small holdings?',\n",
              " 'How does the Tea Subsidy Act contribute to the marketing and manufacturing of tea products?']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization for Evaluation"
      ],
      "metadata": {
        "id": "3HdH7Rxo03gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4 for evaluating the responses\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "\n",
        "from llama_index.core import Settings\n",
        "Settings.llm = gpt4\n",
        "\n",
        "# Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
        "faithfulness_gpt4 = FaithfulnessEvaluator()\n",
        "relevancy_gpt4 = RelevancyEvaluator()\n"
      ],
      "metadata": {
        "id": "PmuhVO310uiz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define function to evaluate metrics like response time, faithfulness and relevancy for a given chunk size"
      ],
      "metadata": {
        "id": "z_8BSiEA2FuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
        "# GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n",
        "def evaluate_response_time_faithfulness_relevancy(chunk_size, eval_questions):\n",
        "\n",
        "    total_response_time = 0\n",
        "    total_faithfulness = 0\n",
        "    total_relevancy = 0\n",
        "\n",
        "    # create vector index\n",
        "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    Settings.chunk_size = chunk_size\n",
        "    Settings.llm = llm\n",
        "    Settings.api_key = openai.api_key\n",
        "\n",
        "    vector_index = VectorStoreIndex.from_documents(eval_documents)\n",
        "\n",
        "    # By default, similarity_top_k is set to 2. To experiment with different values, pass it as an argument to as_query_engine()\n",
        "    query_engine = vector_index.as_query_engine()\n",
        "    num_questions = len(eval_questions)\n",
        "\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    for question in eval_questions:\n",
        "        start_time = time.time()\n",
        "        response_vector = query_engine.query(question)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
        "            query=question, response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        total_response_time += elapsed_time\n",
        "        total_faithfulness += faithfulness_result\n",
        "        total_relevancy += relevancy_result\n",
        "\n",
        "    average_response_time = total_response_time / num_questions\n",
        "    average_faithfulness = total_faithfulness / num_questions\n",
        "    average_relevancy = total_relevancy / num_questions\n",
        "\n",
        "    return average_response_time, average_faithfulness, average_relevancy"
      ],
      "metadata": {
        "id": "zkwFFWj51mFy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate over different chunk sizes to evaluate the metrics"
      ],
      "metadata": {
        "id": "4Cmj8YhI8CZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over different chunk sizes to evaluate the metrics\n",
        "\n",
        "for chunk_size in [256, 512, 1024, 2048]:\n",
        "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_faithfulness_relevancy(chunk_size, eval_questions)\n",
        "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ub6FJef3tv3",
        "outputId": "89368c12-9a66-48e3-8493-833407fb5a4b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk size 256 - Average Response time: 2.88s, Average Faithfulness: 0.80, Average Relevancy: 0.80\n",
            "Chunk size 512 - Average Response time: 1.31s, Average Faithfulness: 1.00, Average Relevancy: 1.00\n",
            "Chunk size 1024 - Average Response time: 1.05s, Average Faithfulness: 0.90, Average Relevancy: 0.90\n",
            "Chunk size 2048 - Average Response time: 1.19s, Average Faithfulness: 0.90, Average Relevancy: 0.90\n"
          ]
        }
      ]
    }
  ]
}